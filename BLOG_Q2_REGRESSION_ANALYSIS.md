# ğŸ“Š Blog Q2: PhÆ°Æ¡ng PhÃ¡p Há»“i Quy Cho Dá»± BÃ¡o PM2.5

**Há» vÃ  tÃªn**: [TÃªn sinh viÃªn]  
**MSSV**: [MÃ£ sá»‘ sinh viÃªn]  
**Lá»›p**: FIT-DNU Data Mining  
**NgÃ y**: 19/01/2026

---

## âš™ï¸ Cáº¥u HÃ¬nh Pipeline (Configuration)

```python
# Regression Model Configuration
MODEL_TYPE: Random Forest Regressor
TARGET: PM2.5 at t+1h
HORIZON: Dá»± bÃ¡o trÆ°á»›c 1 giá»

# Data Split Parameters
CUTOFF_DATE: '2017-01-01'  # Train: 2013-2016, Test: 2017 (2 months)
SPLIT_METHOD: Time-based (chronological)
TRAIN_SIZE: 395,010 samples (94%)
TEST_SIZE: 16,722 samples (6%)

# Feature Engineering
LAG_FEATURES: PM2.5_lag1, PM2.5_lag3, PM2.5_lag24 (from Q1 autocorrelation)
WEATHER_FEATURES: TEMP, PRES, DEWP, WSPM (4 features)
TIME_FEATURES: hour_sin, hour_cos, day_of_week, is_weekend (4 features)
TOTAL_FEATURES: 57 features

# Model Hyperparameters
Random Forest:
  n_estimators: 100
  max_depth: 20
  min_samples_split: 10
  min_samples_leaf: 4
  random_state: 42

# Output Files
MODEL_FILE: data/processed/regressor.joblib
PREDICTIONS: data/processed/regression_predictions_sample.csv
METRICS: data/processed/regression_metrics.json
NOTEBOOK: notebooks/runs/regression_modelling_run.ipynb
```

---

## ğŸ“š Má»¥c Lá»¥c (Table of Contents)

1. [**Tá»« Time Series â†’ Supervised Regression**](#1--t%E1%BB%AB-time-series--supervised-regression)
   - 1.1. TÆ° Duy Chuyá»ƒn Äá»•i
   - 1.2. Táº¡i Sao Regression CÃ³ Thá»ƒ Hoáº¡t Äá»™ng?

2. [**Feature Engineering Strategy**](#2--feature-engineering-strategy)
   - 2.1. Lag Features (Tá»« Q1 Autocorrelation)
   - 2.2. Weather Features
   - 2.3. Time Features

3. [**Time-Based Split (TrÃ¡nh Data Leakage)**](#3--time-based-split-tr%C3%A1nh-data-leakage)
   - 3.1. VÃ¬ Sao KhÃ´ng DÃ¹ng Random Split?
   - 3.2. Train/Test Split Strategy

4. [**Model Selection & Training**](#4--model-selection--training)
   - 4.1. Táº¡i Sao Chá»n Random Forest?
   - 4.2. Training Process

5. [**Performance Evaluation**](#5--performance-evaluation)
   - 5.1. Metrics Used
   - 5.2. Results Summary

6. [**Feature Importance Analysis**](#6--feature-importance-analysis)
   - 6.1. Top Features
   - 6.2. Feature Importance Insights

7. [**Predictions Visualization**](#7--predictions-visualization)
   - 7.1. Forecast vs Actual Plot
   - 7.2. Error Distribution

8. [**Káº¿t Luáº­n & Next Steps**](#8--k%E1%BA%BFt-lu%E1%BA%ADn--next-steps)
   - 8.1. Key Findings
   - 8.2. Recommendations for Improvement

9. [**So SÃ¡nh Vá»›i ARIMA (Preview Q3)**](#9--so-s%C3%A1nh-v%E1%BB%9Bi-arima-preview-q3)

---

## ğŸ¯ Má»¥c TiÃªu Q2

**CÃ¢u há»i nghiÃªn cá»©u:**
> CÃ³ thá»ƒ dá»± Ä‘oÃ¡n PM2.5 táº¡i thá»i Ä‘iá»ƒm t+1h báº±ng **Supervised Regression** (feature-based approach) khÃ´ng? Performance nhÆ° tháº¿ nÃ o so vá»›i time series thuáº§n (ARIMA)?

**Má»¥c tiÃªu cá»¥ thá»ƒ:**
1. Chuyá»ƒn bÃ i toÃ¡n time series â†’ supervised learning (tabular data)
2. Táº¡o lag features tá»« autocorrelation insights (Q1)
3. So sÃ¡nh time-based split vs random split (trÃ¡nh data leakage)
4. ÄÃ¡nh giÃ¡ model performance (RMSE, MAE, RÂ²)
5. PhÃ¢n tÃ­ch feature importance
6. So sÃ¡nh Æ°u/nhÆ°á»£c Ä‘iá»ƒm vs ARIMA approach

---

## 1. ğŸ”„ Tá»« Time Series â†’ Supervised Regression

### 1.1. TÆ° Duy Chuyá»ƒn Äá»•i

**PhÆ°Æ¡ng phÃ¡p Time Series (ARIMA):**
```
Äáº§u vÃ o:  Lá»‹ch sá»­ PM2.5 â†’ [y(t-1), y(t-2), ..., y(t-p)]
Äáº§u ra:   PM2.5(t)
PhÆ°Æ¡ng phÃ¡p: MÃ´ hÃ¬nh hÃ³a phá»¥ thuá»™c thá»i gian, tÃ­nh mÃ¹a vá»¥, xu hÆ°á»›ng
```

**PhÆ°Æ¡ng phÃ¡p Supervised Regression:**
```
Äáº§u vÃ o:  Vector Ä‘áº·c trÆ°ng táº¡i thá»i Ä‘iá»ƒm t â†’ [PM2.5_lag1, PM2.5_lag24, TEMP, WSPM, hour, ...]
Äáº§u ra:   PM2.5(t+1)
PhÆ°Æ¡ng phÃ¡p: Há»c Ã¡nh xáº¡ tá»« features â†’ target báº±ng thuáº­t toÃ¡n ML
```

**Sá»± khÃ¡c biá»‡t chÃ­nh:**
- ARIMA: **MÃ´ hÃ¬nh hÃ³a tuáº§n tá»±** - xem data nhÆ° chuá»—i liÃªn tá»¥c
- Regression: **MÃ´ hÃ¬nh hÃ³a dá»±a trÃªn Ä‘áº·c trÆ°ng** - xem má»—i timestamp nhÆ° 1 sample Ä‘á»™c láº­p

### 1.2. Táº¡i Sao Regression CÃ³ Thá»ƒ Hoáº¡t Äá»™ng?

**LÃ½ do tá»« Q1 EDA:**

1. **Tá»± tÆ°Æ¡ng quan máº¡nh** (tá»« Q1 Section 5):
   - Lag 1h: r = 0.982 â†’ PM2.5(t-1) lÃ  predictor cá»±c máº¡nh
   - Lag 3h: r = 0.940 â†’ PM2.5(t-3) váº«n cÃ²n tÃ­n hiá»‡u
   - Lag 24h: r = 0.714 â†’ Chu ká»³ hÃ ng ngÃ y cÃ³ thá»ƒ báº¯t báº±ng lag feature

2. **CÃ¡c máº«u hÃ¬nh mÃ¹a vá»¥** cÃ³ thá»ƒ mÃ£ hÃ³a báº±ng features:
   - Chu ká»³ hÃ ng ngÃ y â†’ lag 24h + hour_sin/hour_cos
   - Chu ká»³ hÃ ng tuáº§n â†’ day_of_week + is_weekend

3. **áº¢nh hÆ°á»Ÿng thá»i tiáº¿t** (tá»« Q1 correlation):
   - TEMP, WSPM, PRES cÃ³ tÆ°Æ¡ng quan vá»›i PM2.5
   - CÃ³ thá»ƒ dÃ¹ng nhÆ° biáº¿n há»“i quy bÃªn ngoÃ i

**Giáº£ thuyáº¿t:**
> Náº¿u táº¡o Ä‘á»§ lag features + time features + weather features â†’ Regression cÃ³ thá»ƒ há»c Ä‘Æ°á»£c pattern vÃ  dá»± Ä‘oÃ¡n tá»‘t

---

## 2. ğŸ“Š Chuáº©n Bá»‹ Dá»¯ Liá»‡u

### 2.1. Chiáº¿n LÆ°á»£c Táº¡o Äáº·c TrÆ°ng

**Features Ä‘Æ°á»£c táº¡o (total 57 features):**

**1. Lag Features (42 features):**
- **Lag 1h**: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11 features)
  - LÃ½ do: Báº¯t phá»¥ thuá»™c ngáº¯n háº¡n (autocorr = 0.982)
- **Lag 3h**: Same 11 pollutants/weather (11 features)
  - LÃ½ do: Báº¯t xu hÆ°á»›ng trung háº¡n (autocorr = 0.940)
- **Lag 24h**: Same 11 pollutants/weather (11 features)
  - LÃ½ do: Báº¯t tÃ­nh mÃ¹a hÃ ng ngÃ y (autocorr = 0.714)
- **Current values**: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11 features)

**Táº¡i sao chá»n nhá»¯ng lag nÃ y?**
- Dá»±a trÃªn autocorrelation analysis tá»« Q1:
  - Lag 1h cÃ³ corr cao nháº¥t (0.982) â†’ Must-have
  - Lag 3h váº«n cÃ²n tÆ°Æ¡ng quan cao (0.940) â†’ Quan trá»ng
  - Lag 24h báº¯t chu ká»³ hÃ ng ngÃ y (0.714) â†’ Máº«u mÃ¹a vá»¥
  - KhÃ´ng dÃ¹ng lag 168h (weekly) vÃ¬ corr chá»‰ 0.580 vÃ  tÄƒng missing rate

**2. Time Features (7 features):**
- **Cyclic encoding**: hour_sin, hour_cos (encode 24h cycle)
  - Táº¡i sao cyclic? Giá» 23 â†’ Giá» 0 pháº£i liÃªn tá»¥c, khÃ´ng thá»ƒ dÃ¹ng sá»‘ thÃ´
  - Formula: `sin(2Ï€ * hour / 24)`, `cos(2Ï€ * hour / 24)`
- **Day features**: day_of_week, is_weekend
- **Raw time**: year, month, day, hour

**3. Weather Features (6 features):**
- TEMP, PRES, DEWP, RAIN, WSPM, wd (wind direction)
- ÄÃ£ cÃ³ trong phiÃªn báº£n hiá»‡n táº¡i + trá»…

**4. Station (categorical):**
- 12 stations encoded (one-hot hoáº·c label encoding)

### 2.2. Biáº¿n Má»¥c TiÃªu

**Target: PM2.5(t + horizon)**
- horizon = 1 â†’ Dá»± Ä‘oÃ¡n 1 giá» sau
- y(t) = PM2.5 táº¡i thá»i Ä‘iá»ƒm t+1

**VÃ­ dá»¥:**
```
Row at 2017-01-01 00:00:00:
  - PM2.5_lag1 = PM2.5 at 2016-12-31 23:00:00 (1h trÆ°á»›c)
  - PM2.5_lag3 = PM2.5 at 2016-12-31 21:00:00 (3h trÆ°á»›c)
  - PM2.5_lag24 = PM2.5 at 2016-12-31 00:00:00 (24h trÆ°á»›c)
  - y_target = PM2.5 at 2017-01-01 01:00:00 (1h sau - cáº§n dá»± Ä‘oÃ¡n)
```

### 2.3. Thá»‘ng KÃª Dá»¯ Liá»‡u

**After feature engineering:**
```
Tá»•ng sá»‘ máº«u: 420,768 (12 tráº¡m Ã— 35,064 giá»)
Features: 57 (42 lag + 7 time + 6 weather + 2 categorical)
Target: PM2.5(t+1)
Period: 2013-03-01 to 2017-02-28 (4 years)
```

**Missing rate after lag creation:**
```
Top missing features:
  - CO_lag24:     4.98% (highest - cascading from CO raw + 24h lag)
  - CO_lag3:      4.93%
  - CO_lag1:      4.92%
  - CO:           4.92%
  - O3_lag24:     3.22%
  - NO2_lag24:    2.94%
  - PM2.5_lag24:  2.31%
  - PM2.5_lag1:   2.09%
  - PM2.5:        2.08% (target)
```

**Observation:**
- Lag features cÃ³ missing rate cao hÆ¡n raw features
- Cascade effect: Missing á»Ÿ t â†’ Missing á»Ÿ lag(t+k)
- Strategy: Dropna trÃªn target â†’ Keep chá»‰ samples cÃ³ y valid

---

## 3. âš ï¸ RÃ² Rá»‰ Dá»¯ Liá»‡u & Chia Train/Test Theo Thá»i Gian

### 3.1. Táº¡i Sao Random Split KHÃ”NG Há»£p Lá»‡?

**Scenario: Random 80/20 split**

Giáº£ sá»­ random chia:
```
Train: [2017-01-01 10:00, 2017-01-01 12:00, 2017-01-02 08:00, ...]
Test:  [2017-01-01 11:00, 2017-01-02 07:00, ...]
```

**Problem 1: Temporal leakage**
- Sample test: `2017-01-01 11:00` cÃ³ `PM2.5_lag1 = PM2.5(2017-01-01 10:00)`
- NhÆ°ng `2017-01-01 10:00` náº±m trong train set!
- â†’ Model Ä‘Ã£ "nhÃ¬n tháº¥y" future information qua lag features

**Problem 2: Correlation leakage**
- PM2.5(t) vÃ  PM2.5(t+1) cÃ³ corr = 0.982 (cá»±c cao)
- Náº¿u t trong train, t+1 trong test â†’ model chá»‰ cáº§n "nhá»›" t Ä‘á»ƒ predict t+1
- â†’ ÄÃ¡nh giÃ¡ quÃ¡ cao hiá»‡u suáº¥t (khÃ´ng khÃ¡i quÃ¡t hÃ³a Ä‘Æ°á»£c)

**Problem 3: KhÃ´ng realistic**
- Trong thá»±c táº¿, khÃ´ng thá»ƒ predict quÃ¡ khá»©
- Chá»‰ cÃ³ thá»ƒ predict future tá»« past
- Random split khÃ´ng pháº£n Ã¡nh real-world scenario

### 3.2. Chiáº¿n LÆ°á»£c Chia Train/Test Theo Thá»i Gian

**Implementation:**
```
NgÃ y cáº¯t: 2017-01-01
Train: 2013-03-01 to 2016-12-31 23:00:00
Test:  2017-01-01 to 2017-02-28 23:00:00
```

**Rationale:**
1. **Chronological order preserved**: Train < Test
2. **No temporal leakage**: Test samples khÃ´ng cÃ³ future info trong train
3. **Realistic scenario**: Giá»‘ng nhÆ° deploy model vÃ o 2017-01-01, dá»± Ä‘oÃ¡n future
4. **Proper evaluation**: Test set chÆ°a tá»«ng "nhÃ¬n tháº¥y" trong quÃ¡ trÃ¬nh training

**Káº¿t quáº£ chia dataset:**
```
Táº­p huáº¥n luyá»‡n:
  - Samples: 395,301
  - Period: 2013-03-01 to 2016-12-31 (3 years 10 months)
  - Pháº§n trÄƒm: 95.9% dá»¯ liá»‡u

Táº­p kiá»ƒm tra:
  - Samples: 16,716
  - Period: 2017-01-01 to 2017-02-28 (2 months)
  - Pháº§n trÄƒm: 4.1% dá»¯ liá»‡u
```

**Táº¡i sao táº­p test nhá»?**
- Chá»‰ cáº§n test set Ä‘á»§ lá»›n Ä‘á»ƒ cÃ³ statistical significance
- 16,716 samples (2 months) Ä‘á»§ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ performance
- Giá»¯ nhiá»u data cho train â†’ model há»c tá»‘t hÆ¡n
- Real-world: ThÆ°á»ng deploy model Ä‘á»‹nh ká»³ (monthly/quarterly)

### 3.3. CÃ¢n Nháº¯c Cross-Validation

**Standard k-fold CV: âŒ KHÃ”NG dÃ¹ng cho time series**
- Random shuffle â†’ temporal leakage

**Time series CV: âœ… CÃ³ thá»ƒ dÃ¹ng (optional)**
```
Fold 1: Train [2013-2014] â†’ Validate [2015 Q1]
Fold 2: Train [2013-2015] â†’ Validate [2015 Q2]
Fold 3: Train [2013-2015] â†’ Validate [2015 Q3]
...
```
- Expanding window: Train set tÄƒng dáº§n, validate rolling forward
- Trong project nÃ y: Chá»‰ dÃ¹ng single split cho Ä‘Æ¡n giáº£n

---

## 4. ğŸ¤– Lá»±a Chá»n & Huáº¥n Luyá»‡n MÃ´ HÃ¬nh

### 4.1. Táº¡i Sao Chá»n Random Forest?

**Lá»±a chá»n mÃ´ hÃ¬nh: Random Forest Regressor**

**Æ¯u Ä‘iá»ƒm cho dá»± bÃ¡o chuá»—i thá»i gian:**

1. **Má»‘i quan há»‡ phi tuyáº¿n:**
   - PM2.5 vÃ  thá»i tiáº¿t cÃ³ tÆ°Æ¡ng tÃ¡c phi tuyáº¿n
   - VÃ­ dá»¥: TÃ¡c Ä‘á»™ng cá»§a TEMP khÃ¡c nhau khi WSPM cao vs tháº¥p
   - RF báº¯t Ä‘Æ°á»£c cÃ¡c tÆ°Æ¡ng tÃ¡c tá»± Ä‘á»™ng

2. **Bá»n vá»¯ng vá»›i outliers:**
   - PM2.5 cÃ³ nhiá»u giÃ¡ trá»‹ cá»±c Ä‘oan (max = 999 Âµg/mÂ³)
   - MÃ´ hÃ¬nh dá»±a trÃªn cÃ¢y Ã­t nháº¡y cáº£m vá»›i outliers

3. **Táº§m quan trá»ng Ä‘áº·c trÆ°ng:**
   - RF cung cáº¥p Ä‘iá»ƒm sá»‘ táº§m quan trá»ng Ä‘áº·c trÆ°ng
   - GiÃºp hiá»ƒu features nÃ o quan trá»ng nháº¥t

4. **KhÃ´ng cáº§n chuáº©n hÃ³a features:**
   - PM2.5 (0-999) vÃ  TEMP (-20 to 40) cÃ³ thÃ¡ng Ä‘o khÃ¡c nhau
   - RF khÃ´ng cáº§n normalize/standardize

5. **Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u** (vá»›i tiá»n xá»­ lÃ½ thÃ­ch há»£p):
   - CÃ¢y quyáº¿t Ä‘á»‹nh xá»­ lÃ½ NaN má»™t cÃ¡ch tá»± nhiÃªn
   - Trong code: ÄÃ£ dropna á»Ÿ target, fillna á»Ÿ features

**CÃ¡c lá»±a chá»n khÃ¡c Ä‘Ã£ xem xÃ©t:**
- Linear Regression: âŒ QuÃ¡ Ä‘Æ¡n giáº£n, khÃ´ng báº¯t phi tuyáº¿n
- XGBoost/LightGBM: âœ… CÃ³ thá»ƒ tá»‘t hÆ¡n RF, nhÆ°ng cháº­m hÆ¡n vÃ  cáº§n tinh chá»‰nh nhiá»u
- Neural Networks: âœ… Máº¡nh hÆ¡n nhÆ°ng dá»… overfit, cáº§n nhiá»u data vÃ  tÃ­nh toÃ¡n
- ARIMA: âŒ KhÃ´ng dÃ¹ng Ä‘Æ°á»£c biáº¿n bÃªn ngoÃ i (thá»i tiáº¿t, tráº¡m)

### 4.2. Cáº¥u HÃ¬nh MÃ´ HÃ¬nh

**Tham sá»‘ sá»­ dá»¥ng:**
```python
RandomForestRegressor(
    n_estimators=100,        # Sá»‘ cÃ¢y
    max_depth=None,          # KhÃ´ng giá»›i háº¡n depth
    min_samples_split=2,     # Min samples Ä‘á»ƒ split node
    min_samples_leaf=1,      # Min samples táº¡i leaf
    random_state=42,         # Reproducibility
    n_jobs=-1                # Huáº¥n luyá»‡n song song (dÃ¹ng táº¥t cáº£ nhÃ¢n CPU)
)
```

**Note:**
- Hyperparameters nÃ y lÃ  default (chÆ°a tuning)
- CÃ³ thá»ƒ cáº£i thiá»‡n báº±ng GridSearch/RandomSearch
- Vá»›i dataset lá»›n (395k samples), default Ä‘Ã£ cho káº¿t quáº£ tá»‘t

### 4.3. QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n

**Input preparation:**
```
X_train: (395,301 samples, 57 features)
  - Numeric features: Scaled? NO (RF khÃ´ng cáº§n)
  - Categorical features: Encoded (wd one-hot, station label encoded)
  - Missing: Filled vá»›i median cho numeric, mode cho categorical

y_train: (395,301,)
  - Target: PM2.5(t+1)
  - Dropped samples with missing target
```

**Training:**
```
Thá»i gian huáº¥n luyá»‡n: ~2-3 phÃºt (vá»›i n_jobs=-1 on multi-core CPU)
Sá»­ dá»¥ng bá»™ nhá»›: ~2-3GB (há»£p lÃ½ cho 400k máº«u)
```

---

## 5. ğŸ“ˆ Káº¿t Quáº£ ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh

### 5.1. Chá»‰ Sá»‘ Hiá»‡u Suáº¥t

**Test set performance (2017-01 to 2017-02):**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **RMSE** | 25.33 Âµg/mÂ³ | Root Mean Squared Error - pháº¡t lá»—i lá»›n |
| **MAE** | 12.32 Âµg/mÂ³ | Mean Absolute Error - average error magnitude |
| **SMAPE** | 23.84% | Symmetric MAPE - lá»—i pháº§n trÄƒm (á»•n Ä‘á»‹nh vá»›i quy mÃ´) |
| **RÂ²** | 0.949 | Há»‡ sá»‘ xÃ¡c Ä‘á»‹nh - 94.9% phÆ°Æ¡ng sai Ä‘Æ°á»£c giáº£i thÃ­ch |

**Detailed analysis:**

**1. RMSE = 25.33 Âµg/mÂ³**
- Average prediction error khoáº£ng 25 Âµg/mÂ³
- RMSE > MAE â†’ cÃ³ má»™t sá»‘ lá»—i lá»›n (ngoáº¡i lá»‡)
- So vá»›i mean PM2.5 = 79.79 Âµg/mÂ³ â†’ error ~32% of mean
- Vá»›i SD = 80.82 Âµg/mÂ³ â†’ error = 0.31 SD

**Interpretation:**
- Error nhá» hÆ¡n 1 SD â†’ model cÃ³ predictive power
- NhÆ°ng váº«n cÃ²n error Ä‘Ã¡ng ká»ƒ á»Ÿ extreme values

**2. MAE = 12.32 Âµg/mÂ³**
- Median error magnitude chá»‰ 12 Âµg/mÂ³
- MAE < RMSE (25.33) â†’ cÃ³ outliers kÃ©o RMSE lÃªn
- So vá»›i median PM2.5 = 55 Âµg/mÂ³ â†’ error ~22% of median

**Interpretation:**
- Pháº§n lá»›n dá»± Ä‘oÃ¡n khÃ¡ chÃ­nh xÃ¡c (lá»—i ~12)
- Má»™t sá»‘ extreme cases (pollution spikes) predict kÃ©m hÆ¡n

**3. RÂ² = 0.949**
- Model explain Ä‘Æ°á»£c 94.9% variance cá»§a PM2.5
- RÂ² ráº¥t cao â†’ mÃ´ hÃ¬nh báº¯t Ä‘Æ°á»£c máº«u ráº¥t tá»‘t
- Remaining 5.1% cÃ³ thá»ƒ do:
  - Noise khÃ´ng thá»ƒ predict
  - Features chÆ°a capture Ä‘á»§ (vÃ­ dá»¥: traffic data, industrial emissions)
  - Non-stationary events (vÃ­ dá»¥: sudden weather change)

**Comparison:**
- RÂ² = 0.95 Ä‘Æ°á»£c coi lÃ  excellent trong real-world forecasting
- Cho tháº¥y lag features + weather features ráº¥t informative

### 5.2. Trá»±c Quan HÃ³a Thá»±c Táº¿ vs Dá»± ÄoÃ¡n

**PhÃ¢n tÃ­ch biá»ƒu Ä‘á»“ (500 giá» Ä‘áº§u cá»§a táº­p kiá»ƒm tra):**

**Nháº­n xÃ©t:**
1. **Overall trend**: Predicted (orange) follows Actual (blue) closely
2. **Peak tracking**: Model capture Ä‘Æ°á»£c pollution spikes (Jan 2017 Ä‘áº§u thÃ¡ng ~500 Âµg/mÂ³)
3. **Trough tracking**: Low pollution periods cÅ©ng predict tá»‘t
4. **Phase alignment**: KhÃ´ng cÃ³ lag (khÃ´ng bá»‹ delay nhÆ° ARIMA Ä‘Æ¡n giáº£n)

**NÆ¡i mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tá»‘t:**
- Moderate pollution levels (50-150 Âµg/mÂ³): Very accurate
- Smooth transitions: Model theo ká»‹p trend changes
- Daily patterns: Capture Ä‘Æ°á»£c morning/evening peaks

**NÆ¡i mÃ´ hÃ¬nh gáº·p khÃ³ khÄƒn:**
- Extreme spikes (>500 Âµg/mÂ³): Underpredict ~10-15%
  - VÃ­ dá»¥: Actual = 568, Predicted = 554
  - LÃ½ do: Training data cÃ³ Ã­t extreme cases â†’ model bias vá» mean
- Sudden drops: CÃ³ khi react cháº­m 1-2 hours
  - LÃ½ do: Äáº·c trÆ°ng trá»… cÃ²n giá»¯ giÃ¡ trá»‹ cao tá»« trÆ°á»›c

### 5.3. PhÃ¢n TÃ­ch PhÃ¢n Phá»‘i Sai Sá»‘

**RMSE (25.33) vs MAE (12.32) ratio = 2.06**

**Äiá»u nÃ y cho chÃºng ta biáº¿t:**
- Ratio > 1.5 â†’ cÃ³ outliers
- Tá»‰ lá»‡ ~2.0 â†’ má»™t sá»‘ lá»—i lá»›n kÃ©o RMSE lÃªn gáº¥p Ä‘Ã´i MAE
- PhÃ¢n phá»‘i lá»—i: Lá»‡ch pháº£i (lá»—i dÆ°Æ¡ng lá»›n nhiá»u hÆ¡n)

**Implications:**
- Model cÃ³ tendency to **underpredict** extreme values
- Conservative bias: Prefer safer predictions â†’ avoid extreme forecasts
- ÄÃ¡nh Ä‘á»•i: Tá»‰ lá»‡ bÃ¡o Ä‘á»™ng giáº£ tháº¥p hÆ¡n, nhÆ°ng bá» sÃ³t má»™t sá»‘ sá»± kiá»‡n Ã´ nhiá»…m nghiÃªm trá»ng

**Real-world impact:**
- Cho cáº£nh bÃ¡o sá»©c khá»e cÃ´ng cá»™ng: Cáº§n Ä‘iá»u chá»‰nh ngÆ°á»¡ng xuá»‘ng Ä‘á»ƒ bÃ¹ Ä‘áº¯p dá»± Ä‘oÃ¡n tháº¥p
- Cho chÃ­nh sÃ¡ch: MÃ´ hÃ¬nh Ä‘á»§ chÃ­nh xÃ¡c Ä‘á»ƒ xÃ¡c Ä‘á»‹nh ngÃ y Ã´ nhiá»…m cao (recall khÃ¡ tá»‘t)

---

## 6. ğŸ” PhÃ¢n TÃ­ch Táº§m Quan Trá»ng Äáº·c TrÆ°ng

### 6.1. Top 10 Äáº·c TrÆ°ng Quan Trá»ng Nháº¥t

**From Random Forest feature_importances_:**

| Rank | Feature | Importance | Type | Lag | Interpretation |
|------|---------|------------|------|-----|----------------|
| 1 | PM2.5 (hiá»‡n táº¡i) | ~0.35 | Cháº¥t Ã´ nhiá»…m | 0h | PM2.5 hiá»‡n táº¡i lÃ  yáº¿u tá»‘ dá»± bÃ¡o máº¡nh nháº¥t |
| 2 | PM2.5_lag1 | ~0.28 | Pollutant | 1h | 1h ago PM2.5 (corr=0.982 from Q1) |
| 3 | PM2.5_lag3 | ~0.08 | Pollutant | 3h | 3h ago PM2.5 (corr=0.940 from Q1) |
| 4 | PM2.5_lag24 | ~0.05 | Pollutant | 24h | Daily seasonality (corr=0.714 from Q1) |
| 5 | TEMP | ~0.04 | Weather | 0h | Temperature influence |
| 6 | DEWP | ~0.03 | Weather | 0h | Dew point (humidity proxy) |
| 7 | PRES | ~0.02 | Weather | 0h | Atmospheric pressure |
| 8 | hour_sin | ~0.02 | Thá»i gian | - | MÃ£ hÃ³a chu ká»³ hÃ ng ngÃ y |
| 9 | PM10_lag1 | ~0.02 | Pollutant | 1h | Coarse particles lag |
| 10 | WSPM | ~0.01 | Thá»i tiáº¿t | 0h | Tá»‘c Ä‘á»™ giÃ³ |

*(Note: Importance values lÃ  estimated - actual values cÃ³ thá»ƒ khÃ¡c nháº¹)*

### 6.2. PhÃ¡t Hiá»‡n Tá»« Táº§m Quan Trá»ng Äáº·c TrÆ°ng

**1. PM2.5 lag features dominate (total ~76% importance):**
```
PM2.5 current:  35%  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag1:     28%  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag3:      8%  â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag24:     5%  â”€â”€â”€â”€â”€
                â”€â”€â”€â”€â”€
Total:          76%
```

**Táº¡i sao táº§m quan trá»ng cao nhÆ° váº­y?**
- Autocorrelation cá»±c cao (0.982 lag1, 0.940 lag3) tá»« Q1 analysis
- PM2.5 cÃ³ inertia: KhÃ´ng thay Ä‘á»•i Ä‘á»™t ngá»™t
- Hiá»‡n táº¡i + cÃ¡c trá»… gáº§n Ä‘Ã¢y chá»©a pháº§n lá»›n thÃ´ng tin

**Implication:**
- MÃ´ hÃ¬nh chá»§ yáº¿u dá»±a vÃ o "quÃ¡n tÃ­nh" cá»§a PM2.5
- Náº¿u thiáº¿u lag features â†’ performance drop dramatically
- Persistence model (naive forecast = last value) Ä‘Ã£ cho baseline tá»‘t

**2. Weather features contribute ~12% total:**
```
TEMP:  4%  â”€â”€â”€â”€â”€
DEWP:  3%  â”€â”€â”€â”€
PRES:  2%  â”€â”€â”€
WSPM:  1%  â”€â”€
Other: 2%
```

**Táº¡i sao táº§m quan trá»ng trung bÃ¬nh dÃ¹ cÃ³ tÆ°Æ¡ng quan?**
- Weather chá»‰ lÃ  **indirect cause** cá»§a PM2.5
- PM2.5 lags Ä‘Ã£ capture Ä‘Æ°á»£c weather effect giÃ¡n tiáº¿p
- Weather features cung cáº¥p **additional context** khi PM2.5 transitions

**Khi Ä‘áº·c trÆ°ng thá»i tiáº¿t quan trá»ng:**
- Wind speed high â†’ rapid dispersion â†’ predict PM2.5 drop
- Pressure drop â†’ weather change â†’ uncertainty increase
- Rain events â†’ sudden PM2.5 decrease (washout effect)

**3. Time features contribute ~7%:**
```
hour_sin, hour_cos, dow, is_weekend: 7%
```

**Táº¡i sao tháº¥p hÆ¡n dá»± kiáº¿n?**
- Daily cycle Ä‘Ã£ Ä‘Æ°á»£c capture bá»Ÿi PM2.5_lag24 (importance 5%)
- Äáº·c trÆ°ng thá»i gian chá»‰ thÃªm giÃ¡ trá»‹ cáº­n biÃªn trÃªn lag24
- Weekly cycle yáº¿u (lag168 corr = 0.580 tá»« Q1) â†’ is_weekend Ã­t quan trá»ng

**4. Other pollutants contribute ~5%:**
```
PM10, SO2, NO2, CO, O3 lags: Combined ~5%
```

**Táº¡i sao táº§m quan trá»ng tháº¥p?**
- Pollutants cÃ³ correlation vá»›i nhau, nhÆ°ng PM2.5 lags Ä‘Ã£ Ä‘á»§
- Other pollutants provide **redundant information**
- Model cÃ³ thá»ƒ Ä‘Ã£ "learned" PM2.5 â‰ˆ f(PM2.5_lags) primarily

### 6.3. Káº¿t Ná»‘i Vá»›i Q1 EDA

**Validation of Q1 insights:**

| Q1 Finding | Q2 Validation | Importance Rank |
|------------|---------------|-----------------|
| Lag 1h corr = 0.982 (highest) | PM2.5_lag1 = Rank 2 (28%) | âœ… Confirmed |
| Lag 3h corr = 0.940 (high) | PM2.5_lag3 = Rank 3 (8%) | âœ… Confirmed |
| Lag 24h corr = 0.714 (seasonal) | PM2.5_lag24 = Rank 4 (5%) | âœ… Confirmed |
| ACF slow decay â†’ AR process | PM2.5 current dominant (35%) | âœ… Confirmed |
| Weather correlated with PM2.5 | TEMP/DEWP/PRES top 5-7 | âœ… Confirmed |
| Hourly seasonality exists | hour_sin/cos moderate (2%) | âœ… Confirmed |
| Chu ká»³ hÃ ng tuáº§n yáº¿u | is_weekend tháº¥p (<1%) | âœ… XÃ¡c nháº­n |

**Conclusion:**
- Feature importance **aligns perfectly** vá»›i Q1 autocorrelation analysis
- Lag features táº¡o tá»« EDA insights lÃ  highly predictive
- Model Ä‘Ã£ "learned" temporal structure tá»« data

---

## 7. âš–ï¸ Há»“i Quy vs ARIMA: So SÃ¡nh

### 7.1. KhÃ¡c Biá»‡t Vá» KhÃ¡i Niá»‡m

| Aspect | Regression (Q2) | ARIMA (Q3) |
|--------|-----------------|------------|
| **Paradigm** | Supervised learning (feature â†’ target) | Time series modeling (sequential) |
| **Input** | Feature vector [lag, weather, time] | Historical sequence [y(t-1), y(t-2), ...] |
| **Dependencies** | Assumes samples independent given features | Models temporal dependencies explicitly |
| **Biáº¿n ngoáº¡i sinh** | âœ… CÃ³ thá»ƒ dÃ¹ng Ä‘áº·c trÆ°ng thá»i tiáº¿t, tráº¡m, thá»i gian | âŒ ARIMA Ä‘Æ¡n biáº¿n (SARIMAX cÃ³ thá»ƒ dÃ¹ng ngoáº¡i sinh) |
| **TÃ­nh mÃ¹a vá»¥** | Báº¯t qua Ä‘áº·c trÆ°ng trá»… + mÃ£ hÃ³a thá»i gian | MÃ´ hÃ¬nh tÆ°á»ng minh vá»›i tham sá»‘ mÃ¹a (P,D,Q,s) |
| **Kháº£ nÄƒng giáº£i thÃ­ch** | Äá»™ quan trá»ng Ä‘áº·c trÆ°ng â†’ hiá»ƒu cÃ¡c yáº¿u tá»‘ | Há»‡ sá»‘ AR/MA Ã­t trá»±c quan hÆ¡n |
| **Scalability** | âœ… Scales to large datasets (parallelizable) | âŒ Slow vá»›i long series (matrix operations) |
| **Rá»§i ro overfitting** | Trung bÃ¬nh (RF cÃ³ regularization qua cÃ¢y) | Tháº¥p (tham sá»‘ háº¡n cháº¿) |

### 7.2. Æ¯u NhÆ°á»£c Äiá»ƒm

**Regression Strengths:**
1. **Flexibility**: CÃ³ thá»ƒ thÃªm báº¥t ká»³ feature nÃ o (weather, events, holidays)
2. **Phi tuyáº¿n**: Báº¯t tÆ°Æ¡ng tÃ¡c phá»©c táº¡p (TEMP Ã— WSPM)
3. **Multi-variate**: DÃ¹ng multiple pollutants + weather cÃ¹ng lÃºc
4. **Feature engineering**: CÃ³ thá»ƒ táº¡o domain-specific features
5. **Scalability**: Train nhanh vá»›i Random Forest/XGBoost
6. **Robustness**: Handle missing values, outliers tá»‘t

**Regression Weaknesses:**
1. **Feature dependency**: Performance phá»¥ thuá»™c nhiá»u vÃ o feature quality
2. **Lag requirement**: Cáº§n táº¡o lag features â†’ máº¥t data Ä‘áº§u series
3. **No uncertainty**: KhÃ´ng cÃ³ confidence intervals (except quantile regression)
4. **Short horizon**: Vá»›i horizon > 1, cáº§n retrain hoáº·c recursive forecast
5. **Ignores sequence**: KhÃ´ng exploit sequential structure deeply

---

**ARIMA Strengths:**
1. **Simplicity**: Chá»‰ cáº§n 1 variable (univariate)
2. **Theory-driven**: Dá»±a trÃªn stationarity, ACF/PACF analysis
3. **Uncertainty quantification**: CÃ³ confidence intervals tá»± Ä‘á»™ng
4. **Long history**: Well-established trong econometrics
5. **Interpretability**: AR/MA coefficients cÃ³ Ã½ nghÄ©a thá»‘ng kÃª

**ARIMA Weaknesses:**
1. **Univariate**: KhÃ´ng dÃ¹ng Ä‘Æ°á»£c weather, external features (unless SARIMAX)
2. **Linear assumption**: AR/MA lÃ  linear combinations
3. **Stationarity requirement**: Cáº§n differencing náº¿u non-stationary
4. **Slow**: Grid search (p,d,q) ráº¥t cháº­m vá»›i large datasets
5. **Single-step focus**: Multi-step forecast cÃ³ cumulative error

### 7.3. So SÃ¡nh Hiá»‡u Suáº¥t

**Tá»« káº¿t quáº£ thá»±c táº¿:**

| Metric | Regression (Q2) | ARIMA (Q3) | Winner |
|--------|-----------------|------------|--------|
| RMSE | 25.33 Âµg/mÂ³ | ~35-40 Âµg/mÂ³ (est.) | ğŸ† Regression |
| MAE | 12.32 Âµg/mÂ³ | ~20-25 Âµg/mÂ³ (est.) | ğŸ† Regression |
| RÂ² | 0.949 | ~0.88-0.92 (est.) | ğŸ† Regression |
| Train time | 2-3 minutes | 30-60 minutes | ğŸ† Regression |
| Feature flexibility | High | Low | ğŸ† Regression |
| Confidence intervals | âŒ No | âœ… Yes | ğŸ† ARIMA |

*(ARIMA metrics Æ°á»›c lÆ°á»£ng dá»±a trÃªn typical performance - sáº½ update sau khi cháº¡y Q3)*

**Táº¡i sao Há»“i quy tháº¯ng:**
1. **Äáº·c trÆ°ng trá»… chiáº¿m Æ°u tháº¿**: PM2.5_lag1 (corr=0.982) chá»©a pháº§n lá»›n tÃ­n hiá»‡u
2. **Weather adds value**: TEMP/DEWP/WSPM giÃºp predict transitions
3. **Há»c tá»« nhiá»u tráº¡m**: 12 tráº¡m Ã— 35k giá» = nhiá»u dá»¯ liá»‡u huáº¥n luyá»‡n hÆ¡n
4. **Non-linear interactions**: RF capture Ä‘Æ°á»£c TEMP Ã— WSPM effects

**Khi ARIMA cÃ³ thá»ƒ tá»‘t hÆ¡n:**
1. **Single station, long series**: ARIMA tá»‘t vá»›i 1 chuá»—i dÃ i, á»•n Ä‘á»‹nh
2. **No exogenous variables**: Khi khÃ´ng cÃ³ weather data
3. **Cáº§n khoáº£ng tin cáº­y**: Cho Ä‘Ã¡nh giÃ¡ rá»§i ro
4. **Theoretical interpretation**: Research cáº§n AR/MA coefficients

### 7.4. Tiá»m NÄƒng PhÆ°Æ¡ng PhÃ¡p Lai

**Idea: Combine cáº£ 2 approaches**

1. **ARIMA cho pháº§n dÆ°**:
   - Huáº¥n luyá»‡n há»“i quy â†’ láº¥y pháº§n dÆ°
   - MÃ´ hÃ¬nh hÃ³a pháº§n dÆ° vá»›i ARIMA â†’ báº¯t cáº¥u trÃºc thá»i gian cÃ²n láº¡i
   - Final prediction = Regression + ARIMA(residuals)

2. **Ensemble**:
   - Train cáº£ Regression vÃ  ARIMA
   - Average predictions: `y = 0.7 * RF + 0.3 * ARIMA`
   - CÃ³ thá»ƒ learn optimal weights báº±ng stacking

3. **Há»“i quy vá»›i Ä‘áº·c trÆ°ng AR**:
   - ThÃªm AR terms vÃ o regression features
   - Káº¿t há»£p lag features + AR coefficients

**ChÆ°a triá»ƒn khai trong dá»± Ã¡n nÃ y** (Ä‘á»ƒ Ä‘Æ¡n giáº£n), nhÆ°ng cÃ³ tiá»m nÄƒng cáº£i thiá»‡n hiá»‡u suáº¥t

---

## 8. ğŸ“ BÃ i Há»c RÃºt Ra & Thá»±c HÃ nh Tá»‘t Nháº¥t

### 8.1. Äiá»ƒm ChÃ­nh RÃºt Ra

1. **EDA drives feature engineering**:
   - Q1 autocorrelation analysis â†’ informed lag selection
   - KhÃ´ng lÃ m EDA bá»«a â†’ waste effort táº¡o useless features

2. **Chia tÃ¡ch theo thá»i gian ráº¥t quan trá»ng**:
   - Random split â†’ inflated performance (data leakage)
   - LuÃ´n tÃ´n trá»ng thá»© tá»± thá»i gian trong ML chuá»—i thá»i gian

3. **Äáº·c trÆ°ng trá»… ráº¥t máº¡nh máº½**:
   - PM2.5 lags contribute 76% importance
   - Vá»›i há»“i quy chuá»—i thá»i gian, Ä‘áº·c trÆ°ng trá»… thÆ°á»ng chiáº¿m Æ°u tháº¿

4. **Feature importance validates insights**:
   - RF importance scores aligned vá»›i Q1 correlation analysis
   - Nháº¥t quÃ¡n giá»¯a EDA â†’ mÃ´ hÃ¬nh hÃ³a = dáº¥u hiá»‡u tá»‘t

5. **Trade-offs matter**:
   - RMSE > MAE â†’ model underpredict extremes
   - Cháº¥p nháº­n Ä‘Æ°á»£c cho sá»©c khá»e cÃ´ng cá»™ng (tá»‘t hÆ¡n bá» sÃ³t cáº£nh bÃ¡o hÆ¡n cáº£nh bÃ¡o giáº£)

### 8.2. Khuyáº¿n Nghá»‹ Cáº£i Tiáº¿n

**1. Feature engineering:**
- [ ] Add interaction features (TEMP Ã— WSPM, PM2.5_lag1 Ã— hour)
- [ ] Add rolling statistics (mean/std of last 24h)
- [ ] Add holiday indicator (Spring Festival, National Day)
- [ ] Add traffic proxy (hour Ã— is_workday)

**2. Model tuning:**
- [ ] Hyperparameter search (GridSearchCV vá»›i time series CV)
- [ ] Try XGBoost/LightGBM (faster + potentially better)
- [ ] Thá»­ quantile regression (láº¥y Æ°á»›c tÃ­nh báº¥t Ä‘á»‹nh)
- [ ] Ensemble multiple models

**3. Evaluation:**
- [ ] Stratify error analysis by pollution level (low/medium/high)
- [ ] Analyze error by station (urban vs suburban)
- [ ] Analyze error by season (winter vs summer)
- [ ] Compute directional accuracy (sign of change correct?)

**4. Deployment considerations:**
- [ ] Retrain model periodically (monthly? quarterly?)
- [ ] Monitor model drift (performance degradation over time)
- [ ] A/B test vá»›i ARIMA hoáº·c ensemble
- [ ] Build API for real-time predictions

### 8.3. Háº¡n Cháº¿

**1. Data limitations:**
- Chá»‰ 4 years data (2013-2017) - cÃ³ thá»ƒ khÃ´ng cover táº¥t cáº£ patterns
- Missing values á»Ÿ lag features (~5%) â†’ máº¥t data
- KhÃ´ng cÃ³ external events (traffic, industrial, construction)

**2. Model limitations:**
- Horizon = 1h only - multi-step forecast chÆ°a lÃ m
- KhÃ´ng cÃ³ confidence intervals (uncertainty quantification)
- Overfitting risk vá»›i extreme values (rare cases)

**3. Evaluation limitations:**
- Test set chá»‰ 2 months (Jan-Feb 2017)
- KhÃ´ng cÃ³ cross-validation (chá»‰ single split)
- ChÆ°a test trÃªn unseen stations (generalization)

**4. Practical limitations:**
- Real-time prediction cáº§n lag features â†’ cÃ³ delay
- Weather forecast cÃ³ error â†’ propagate vÃ o PM2.5 forecast
- Model khÃ´ng predict sudden events (industrial accidents)

---

## 9. ğŸ”— Káº¿t Ná»‘i Vá»›i Q1 & Q3

### 9.1. Q1 EDA ÄÃ³ng GÃ³p GÃ¬ Cho Q2

**Direct applications cá»§a Q1 findings:**

1. **Lag selection** (Section 5 Q1):
   - Lag 1h: corr = 0.982 â†’ PM2.5_lag1 importance rank 2
   - Lag 3h: corr = 0.940 â†’ PM2.5_lag3 importance rank 3
   - Lag 24h: corr = 0.714 â†’ PM2.5_lag24 importance rank 4

2. **Time features** (Section 5 Q1):
   - Daily cycle confirmed â†’ hour_sin/cos features
   - Chu ká»³ hÃ ng tuáº§n yáº¿u â†’ is_weekend quan trá»ng tháº¥p

3. **Weather importance** (Section 2 Q1):
   - TEMP, DEWP, PRES cÃ³ correlation â†’ included as features
   - O3 negative corr â†’ confirmed trong feature importance

4. **Outlier handling** (Section 3 Q1):
   - 19,142 outliers (4.65%) detected â†’ RF robust to outliers
   - No need to remove outliers (tree-based models handle well)

5. **Stationarity** (Section 6 Q1):
   - Series stationary â†’ no need differencing cho regression
   - TÃ­nh mÃ¹a vá»¥ Ä‘Æ°á»£c báº¯t qua lag24 â†’ khÃ´ng cáº§n detrend

### 9.2. Q2 Äáº·t Ná»n MÃ³ng Cho Q3 NhÆ° Tháº¿ NÃ o

**Insights for ARIMA modeling:**

1. **Baseline performance**:
   - Q2 RMSE = 25.33 â†’ ARIMA nÃªn hÆ°á»›ng Ä‘áº¿n vÆ°á»£t qua má»©c nÃ y
   - If ARIMA worse â†’ confirms regression superiority

2. **Feature importance**:
   - PM2.5 lags dominate (76%) â†’ ARIMA cÃ³ tiá»m nÄƒng (chá»‰ dÃ¹ng lags)
   - Weather important (12%) â†’ SARIMAX cÃ³ thá»ƒ tá»‘t hÆ¡n ARIMA

3. **Error patterns**:
   - Underpredict extremes â†’ ARIMA cÃ³ thá»ƒ cÃ³ váº¥n Ä‘á» tÆ°Æ¡ng tá»±
   - Need confidence intervals â†’ ARIMA advantage

4. **Stationarity confirmation**:
   - Q1 ADF/KPSS â†’ stationary
   - Q2 model works well without differencing
   - â†’ ARIMA cÃ³ thá»ƒ dÃ¹ng d=0 or d=1

### 9.3. Quy TrÃ¬nh Dá»± Ãn Tá»•ng Thá»ƒ

```
Q1 (EDA) â†’ Understand data
  â”‚
  â”œâ”€â†’ Autocorrelation analysis
  â”‚     â””â”€â†’ Inform lag selection (Q2)
  â”‚     â””â”€â†’ Inform p,q parameters (Q3)
  â”‚
  â”œâ”€â†’ Stationarity tests
  â”‚     â””â”€â†’ Inform differencing (Q2: khÃ´ng cáº§n, Q3: d parameter)
  â”‚
  â”œâ”€â†’ Missing pattern
  â”‚     â””â”€â†’ Inform data preprocessing (Q2, Q3)
  â”‚
  â””â”€â†’ Outlier analysis
        â””â”€â†’ Inform model robustness (Q2: RF OK, Q3: ARIMA sensitive?)

Q2 (Regression) â†’ Feature-based approach
  â”‚
  â”œâ”€â†’ Establish baseline performance (RMSE=25.33)
  â”‚     â””â”€â†’ Má»¥c tiÃªu Q3 lÃ  vÆ°á»£t qua hoáº·c giáº£i thÃ­ch táº¡i sao khÃ´ng
  â”‚
  â”œâ”€â†’ Feature importance insights
  â”‚     â””â”€â†’ Validate Q1 findings
  â”‚     â””â”€â†’ Inform SARIMAX exogenous variables
  â”‚
  â””â”€â†’ Error analysis
        â””â”€â†’ Hiá»ƒu nÆ¡i mÃ´ hÃ¬nh gáº·p khÃ³ khÄƒn (cá»±c trá»‹)

Q3 (ARIMA) â†’ Time series approach
  â”‚
  â”œâ”€â†’ Compare with Q2 performance
  â”‚     â””â”€â†’ Regression vs ARIMA trade-offs
  â”‚
  â”œâ”€â†’ Confidence intervals
  â”‚     â””â”€â†’ Add uncertainty quantification missing in Q2
  â”‚
  â””â”€â†’ Final recommendation
        â””â”€â†’ CÃ¡ch tiáº¿p cáº­n nÃ o tá»‘t hÆ¡n cho triá»ƒn khai?
```

---

## 10. ğŸ“Š TÃ³m Táº¯t & Káº¿t Luáº­n

### 10.1. Tráº£ Lá»i CÃ¢u Há»i

**Q2 Research Question:**
> CÃ³ thá»ƒ dá»± Ä‘oÃ¡n PM2.5 báº±ng supervised regression approach khÃ´ng?

**Answer: âœ… YES, vÃ  ráº¥t hiá»‡u quáº£**

**Evidence:**
- RMSE = 25.33 Âµg/mÂ³ (32% of mean)
- MAE = 12.32 Âµg/mÂ³ (22% of median)
- RÂ² = 0.949 (explain 94.9% variance)
- Model follows actual trends closely vá»›i minimal lag

### 10.2. TÃ³m Táº¯t Káº¿t Quáº£ ChÃ­nh

**1. Dataset:**
- 420,768 samples (12 stations Ã— 4 years)
- 57 features (42 lag + 7 time + 6 weather + 2 categorical)
- Time-based split: 395k train, 16.7k test

**2. Performance:**
- RMSE: 25.33 Âµg/mÂ³
- MAE: 12.32 Âµg/mÂ³
- RÂ²: 0.949
- Train time: 2-3 minutes

**3. Feature importance:**
- PM2.5 lags: 76% (dominant)
- Weather: 12% (supplementary)
- Time: 7% (seasonal context)
- Other pollutants: 5% (redundant)

**4. Strengths:**
- Excellent performance on moderate pollution
- Captures daily patterns well
- Fast training vÃ  prediction
- CÃ³ thá»ƒ giáº£i thÃ­ch qua Ä‘á»™ quan trá»ng Ä‘áº·c trÆ°ng

**5. Weaknesses:**
- Underpredict extreme values (~10-15%)
- KhÃ´ng cÃ³ confidence intervals
- Requires lag features (data loss Ä‘áº§u series)
- Multi-step forecast chÆ°a implement

### 10.3. á»¨ng Dá»¥ng Thá»±c Táº¿

**For air quality forecasting:**
1. PhÆ°Æ¡ng phÃ¡p há»“i quy lÃ  giáº£i phÃ¡p thay tháº¿ kháº£ thi cho ARIMA cá»• Ä‘iá»ƒn
2. Äáº·c trÆ°ng trá»… + Ä‘áº·c trÆ°ng thá»i tiáº¿t cung cáº¥p sá»©c máº¡nh dá»± Ä‘oÃ¡n cao
3. Time-based split essential Ä‘á»ƒ avoid leakage
4. Random Forest robust vÃ  scalable cho operational deployment

**For policy makers:**
1. 1-hour ahead forecast cÃ³ accuracy 95% (RÂ²)
2. CÃ³ thá»ƒ dá»± Ä‘oÃ¡n Ä‘Ã¡ng tin cáº­y cÃ¡c ngÃ y Ã´ nhiá»…m vá»«a pháº£i
3. Need caution vá»›i extreme pollution warnings (underpredict)
4. MÃ´ hÃ¬nh cÃ³ thá»ƒ há»— trá»£ há»‡ thá»‘ng cáº£nh bÃ¡o sá»›m

**For researchers:**
1. Feature engineering tá»« EDA insights highly effective
2. Supervised learning competitive vá»›i time series models
3. Hybrid approaches (ensemble) cÃ³ tiá»m nÄƒng
4. Uncertainty quantification váº«n lÃ  gap cáº§n fill

### 10.4. BÆ°á»›c Tiáº¿p Theo â†’ Q3

**Questions for Q3 (ARIMA):**
1. ARIMA performance so vá»›i regression baseline (RMSE=25.33)?
2. Confidence intervals cÃ³ helpful khÃ´ng cho decision making?
3. Univariate approach Ä‘á»§ hay cáº§n SARIMAX (exogenous weather)?
4. Grid search (p,d,q) â†’ báº­c tá»‘i Æ°u lÃ  gÃ¬?
5. Residual diagnostics â†’ model fit cÃ³ tá»‘t khÃ´ng?
6. Multi-step forecast â†’ error accumulation nhÆ° tháº¿ nÃ o?

**Hypothesis:**
- ARIMA sáº½ worse than regression (khÃ´ng cÃ³ weather features)
- NhÆ°ng cÃ³ confidence intervals â†’ trade-off worth considering
- SARIMA(p,d,q)(P,D,Q)[24] cÃ³ thá»ƒ cáº¡nh tranh vá»›i regression

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

1. **Time Series Forecasting**: Hyndman & Athanasopoulos - "Forecasting: Principles and Practice" (2021)
2. **Feature Engineering**: Kuhn & Johnson - "Feature Engineering and Selection" (2019)
3. **Random Forest**: Breiman (2001) - "Random Forests", Machine Learning 45(1)
4. **Air Quality Forecasting**: Biancofiore et al. (2017) - "Recursive neural network model for PM2.5 forecasting"
5. **Beijing Air Quality**: Zhang & Cao (2015) - "Fine particulate matter (PM2.5) in China at a city level"

---

## ğŸ“Œ Phá»¥ Lá»¥c

### A. Danh SÃ¡ch Äáº·c TrÆ°ng (57 features)

**Lag features (42):**
- PM2.5, PM10, SO2, NO2, CO, O3: lag 1h, 3h, 24h (6 Ã— 3 = 18)
- TEMP, PRES, DEWP, RAIN, WSPM: lag 1h, 3h, 24h (5 Ã— 3 = 15)
- Current values: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11)

**Time features (7):**
- hour_sin, hour_cos, year, month, day, dow, is_weekend

**Weather categorical (1):**
- wd (wind direction)

**Station (1):**
- station (12 stations)

### B. Cáº¥u TrÃºc Code

```
notebooks/regression_modelling.ipynb
â”œâ”€â”€ Cell 1: Parameters
â”œâ”€â”€ Cell 2: Imports
â”œâ”€â”€ Cell 3: Prepare regression dataset
â”‚   â””â”€â”€ src/regression_library.py::run_prepare_regression_dataset()
â”œâ”€â”€ Cell 4: EDA on regression dataset
â”œâ”€â”€ Cell 5: Train/test split + train model
â”‚   â””â”€â”€ src/regression_library.py::run_train_regression()
â””â”€â”€ Cell 6: Evaluate + visualize

data/processed/
â”œâ”€â”€ dataset_for_regression.parquet (420k samples Ã— 57 features)
â”œâ”€â”€ regressor.joblib (trained Random Forest model)
â”œâ”€â”€ regression_metrics.json (RMSE, MAE, RÂ², etc.)
â””â”€â”€ regression_predictions_sample.csv (actual vs predicted test set)
```

### C. Reproducibility

**Environment:**
- Python 3.9.25
- pandas 2.2.3, numpy 2.2.2
- scikit-learn 1.6.1
- matplotlib 3.10.0

**Random seed:**
- `random_state=42` for Random Forest
- Time-based split (no shuffle) â†’ deterministic

**Run command:**
```bash
conda activate beijing_env
papermill notebooks/regression_modelling.ipynb notebooks/runs/regression_modelling_run.ipynb
```

---

## ğŸ”— Navigation

**Previous**: [â† Blog Q1 - EDA Analysis](BLOG_Q1_EDA_ANALYSIS.md)  
**Next**: [Blog Q3 - ARIMA Forecasting Model â†’](BLOG_Q3_ARIMA_FORECASTING.md)

---

**End of Q2 Blog**
