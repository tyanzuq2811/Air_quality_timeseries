# ğŸ“Š Blog Q2: Regression Approach for PM2.5 Forecasting

**Há» vÃ  tÃªn**: [TÃªn sinh viÃªn]  
**MSSV**: [MÃ£ sá»‘ sinh viÃªn]  
**Lá»›p**: FIT-DNU Data Mining  
**NgÃ y**: 19/01/2026

---

## ğŸ¯ Má»¥c TiÃªu Q2

**CÃ¢u há»i nghiÃªn cá»©u:**
> CÃ³ thá»ƒ dá»± Ä‘oÃ¡n PM2.5 táº¡i thá»i Ä‘iá»ƒm t+1h báº±ng **Supervised Regression** (feature-based approach) khÃ´ng? Performance nhÆ° tháº¿ nÃ o so vá»›i time series thuáº§n (ARIMA)?

**Má»¥c tiÃªu cá»¥ thá»ƒ:**
1. Chuyá»ƒn bÃ i toÃ¡n time series â†’ supervised learning (tabular data)
2. Táº¡o lag features tá»« autocorrelation insights (Q1)
3. So sÃ¡nh time-based split vs random split (trÃ¡nh data leakage)
4. ÄÃ¡nh giÃ¡ model performance (RMSE, MAE, RÂ²)
5. PhÃ¢n tÃ­ch feature importance
6. So sÃ¡nh Æ°u/nhÆ°á»£c Ä‘iá»ƒm vs ARIMA approach

---

## 1. ğŸ”„ Tá»« Time Series â†’ Supervised Regression

### 1.1. TÆ° Duy Chuyá»ƒn Äá»•i

**Time Series (ARIMA) approach:**
```
Input:  PM2.5 history â†’ [y(t-1), y(t-2), ..., y(t-p)]
Output: PM2.5(t)
Method: Model temporal dependencies, seasonality, trend
```

**Supervised Regression approach:**
```
Input:  Feature vector at time t â†’ [PM2.5_lag1, PM2.5_lag24, TEMP, WSPM, hour, ...]
Output: PM2.5(t+1)
Method: Learn mapping from features â†’ target using ML algorithms
```

**Key difference:**
- ARIMA: **Sequential modeling** - xem data nhÆ° chuá»—i liÃªn tá»¥c
- Regression: **Feature-based modeling** - xem má»—i timestamp nhÆ° 1 sample Ä‘á»™c láº­p

### 1.2. Táº¡i Sao Regression CÃ³ Thá»ƒ Hoáº¡t Äá»™ng?

**LÃ½ do tá»« Q1 EDA:**

1. **Strong autocorrelation** (tá»« Q1 Section 5):
   - Lag 1h: r = 0.982 â†’ PM2.5(t-1) lÃ  predictor cá»±c máº¡nh
   - Lag 3h: r = 0.940 â†’ PM2.5(t-3) váº«n cÃ²n signal
   - Lag 24h: r = 0.714 â†’ Daily seasonality cÃ³ thá»ƒ capture báº±ng lag feature

2. **Seasonality patterns** cÃ³ thá»ƒ encode báº±ng features:
   - Daily cycle â†’ lag 24h + hour_sin/hour_cos
   - Weekly cycle â†’ day_of_week + is_weekend

3. **Weather influence** (tá»« Q1 correlation):
   - TEMP, WSPM, PRES cÃ³ correlation vá»›i PM2.5
   - CÃ³ thá»ƒ dÃ¹ng nhÆ° external regressors

**Hypothesis:**
> Náº¿u táº¡o Ä‘á»§ lag features + time features + weather features â†’ Regression cÃ³ thá»ƒ há»c Ä‘Æ°á»£c pattern vÃ  dá»± Ä‘oÃ¡n tá»‘t

---

## 2. ğŸ“Š Dataset Preparation

### 2.1. Feature Engineering Strategy

**Features Ä‘Æ°á»£c táº¡o (total 57 features):**

**1. Lag Features (42 features):**
- **Lag 1h**: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11 features)
  - Rationale: Capture short-term dependency (autocorr = 0.982)
- **Lag 3h**: Same 11 pollutants/weather (11 features)
  - Rationale: Capture medium-term trend (autocorr = 0.940)
- **Lag 24h**: Same 11 pollutants/weather (11 features)
  - Rationale: Capture daily seasonality (autocorr = 0.714)
- **Current values**: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11 features)

**Why these lags?**
- Dá»±a trÃªn autocorrelation analysis tá»« Q1:
  - Lag 1h cÃ³ corr cao nháº¥t (0.982) â†’ Must-have
  - Lag 3h váº«n cÃ²n high corr (0.940) â†’ Important
  - Lag 24h capture daily cycle (0.714) â†’ Seasonal pattern
  - KhÃ´ng dÃ¹ng lag 168h (weekly) vÃ¬ corr chá»‰ 0.580 vÃ  tÄƒng missing rate

**2. Time Features (7 features):**
- **Cyclic encoding**: hour_sin, hour_cos (encode 24h cycle)
  - Why cyclic? Hour 23 â†’ Hour 0 pháº£i continuous, khÃ´ng thá»ƒ dÃ¹ng raw number
  - Formula: `sin(2Ï€ * hour / 24)`, `cos(2Ï€ * hour / 24)`
- **Day features**: day_of_week, is_weekend
- **Raw time**: year, month, day, hour

**3. Weather Features (6 features):**
- TEMP, PRES, DEWP, RAIN, WSPM, wd (wind direction)
- Already in current + lag versions

**4. Station (categorical):**
- 12 stations encoded (one-hot hoáº·c label encoding)

### 2.2. Target Variable

**Target: PM2.5(t + horizon)**
- horizon = 1 â†’ Dá»± Ä‘oÃ¡n 1 giá» sau
- y(t) = PM2.5 táº¡i thá»i Ä‘iá»ƒm t+1

**Example:**
```
Row at 2017-01-01 00:00:00:
  - PM2.5_lag1 = PM2.5 at 2016-12-31 23:00:00 (1h trÆ°á»›c)
  - PM2.5_lag3 = PM2.5 at 2016-12-31 21:00:00 (3h trÆ°á»›c)
  - PM2.5_lag24 = PM2.5 at 2016-12-31 00:00:00 (24h trÆ°á»›c)
  - y_target = PM2.5 at 2017-01-01 01:00:00 (1h sau - cáº§n dá»± Ä‘oÃ¡n)
```

### 2.3. Dataset Statistics

**After feature engineering:**
```
Total samples: 420,768 (12 stations Ã— 35,064 hours)
Features: 57 (42 lag + 7 time + 6 weather + 2 categorical)
Target: PM2.5(t+1)
Period: 2013-03-01 to 2017-02-28 (4 years)
```

**Missing rate after lag creation:**
```
Top missing features:
  - CO_lag24:     4.98% (highest - cascading from CO raw + 24h lag)
  - CO_lag3:      4.93%
  - CO_lag1:      4.92%
  - CO:           4.92%
  - O3_lag24:     3.22%
  - NO2_lag24:    2.94%
  - PM2.5_lag24:  2.31%
  - PM2.5_lag1:   2.09%
  - PM2.5:        2.08% (target)
```

**Observation:**
- Lag features cÃ³ missing rate cao hÆ¡n raw features
- Cascade effect: Missing á»Ÿ t â†’ Missing á»Ÿ lag(t+k)
- Strategy: Dropna trÃªn target â†’ Keep chá»‰ samples cÃ³ y valid

---

## 3. âš ï¸ Data Leakage & Time-Based Split

### 3.1. Táº¡i Sao Random Split KHÃ”NG Há»£p Lá»‡?

**Scenario: Random 80/20 split**

Giáº£ sá»­ random chia:
```
Train: [2017-01-01 10:00, 2017-01-01 12:00, 2017-01-02 08:00, ...]
Test:  [2017-01-01 11:00, 2017-01-02 07:00, ...]
```

**Problem 1: Temporal leakage**
- Sample test: `2017-01-01 11:00` cÃ³ `PM2.5_lag1 = PM2.5(2017-01-01 10:00)`
- NhÆ°ng `2017-01-01 10:00` náº±m trong train set!
- â†’ Model Ä‘Ã£ "nhÃ¬n tháº¥y" future information qua lag features

**Problem 2: Correlation leakage**
- PM2.5(t) vÃ  PM2.5(t+1) cÃ³ corr = 0.982 (cá»±c cao)
- Náº¿u t trong train, t+1 trong test â†’ model chá»‰ cáº§n "nhá»›" t Ä‘á»ƒ predict t+1
- â†’ Overestimate performance (not generalizable)

**Problem 3: KhÃ´ng realistic**
- Trong thá»±c táº¿, khÃ´ng thá»ƒ predict quÃ¡ khá»©
- Chá»‰ cÃ³ thá»ƒ predict future tá»« past
- Random split khÃ´ng pháº£n Ã¡nh real-world scenario

### 3.2. Time-Based Split Strategy

**Implementation:**
```
Cutoff date: 2017-01-01
Train: 2013-03-01 to 2016-12-31 23:00:00
Test:  2017-01-01 to 2017-02-28 23:00:00
```

**Rationale:**
1. **Chronological order preserved**: Train < Test
2. **No temporal leakage**: Test samples khÃ´ng cÃ³ future info trong train
3. **Realistic scenario**: Giá»‘ng nhÆ° deploy model vÃ o 2017-01-01, dá»± Ä‘oÃ¡n future
4. **Proper evaluation**: Test set chÆ°a tá»«ng "nhÃ¬n tháº¥y" trong quÃ¡ trÃ¬nh training

**Dataset split results:**
```
Train set:
  - Samples: 395,301
  - Period: 2013-03-01 to 2016-12-31 (3 years 10 months)
  - Percentage: 95.9% of data

Test set:
  - Samples: 16,716
  - Period: 2017-01-01 to 2017-02-28 (2 months)
  - Percentage: 4.1% of data
```

**Why test set nhá»?**
- Chá»‰ cáº§n test set Ä‘á»§ lá»›n Ä‘á»ƒ cÃ³ statistical significance
- 16,716 samples (2 months) Ä‘á»§ Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ performance
- Giá»¯ nhiá»u data cho train â†’ model há»c tá»‘t hÆ¡n
- Real-world: ThÆ°á»ng deploy model Ä‘á»‹nh ká»³ (monthly/quarterly)

### 3.3. Cross-Validation Considerations

**Standard k-fold CV: âŒ KHÃ”NG dÃ¹ng cho time series**
- Random shuffle â†’ temporal leakage

**Time series CV: âœ… CÃ³ thá»ƒ dÃ¹ng (optional)**
```
Fold 1: Train [2013-2014] â†’ Validate [2015 Q1]
Fold 2: Train [2013-2015] â†’ Validate [2015 Q2]
Fold 3: Train [2013-2015] â†’ Validate [2015 Q3]
...
```
- Expanding window: Train set tÄƒng dáº§n, validate rolling forward
- Trong project nÃ y: Chá»‰ dÃ¹ng single split cho Ä‘Æ¡n giáº£n

---

## 4. ğŸ¤– Model Selection & Training

### 4.1. Why Random Forest?

**Model choice: Random Forest Regressor**

**Advantages cho time series forecasting:**

1. **Non-linear relationships**:
   - PM2.5 vÃ  weather cÃ³ non-linear interaction
   - Example: TEMP effect khÃ¡c nhau khi WSPM cao vs tháº¥p
   - RF capture Ä‘Æ°á»£c interactions tá»± Ä‘á»™ng

2. **Robust to outliers**:
   - PM2.5 cÃ³ nhiá»u extreme values (max = 999 Âµg/mÂ³)
   - Tree-based models less sensitive to outliers

3. **Feature importance**:
   - RF cung cáº¥p feature importance scores
   - GiÃºp hiá»ƒu features nÃ o quan trá»ng nháº¥t

4. **No feature scaling required**:
   - PM2.5 (0-999) vÃ  TEMP (-20 to 40) cÃ³ scale khÃ¡c nhau
   - RF khÃ´ng cáº§n normalize/standardize

5. **Handles missing values** (vá»›i proper preprocessing):
   - Tree splits handle NaN gracefully
   - Trong code: ÄÃ£ dropna á»Ÿ target, fillna á»Ÿ features

**Alternatives considered:**
- Linear Regression: âŒ QuÃ¡ simple, khÃ´ng capture non-linearity
- XGBoost/LightGBM: âœ… CÃ³ thá»ƒ tá»‘t hÆ¡n RF, nhÆ°ng slower vÃ  cáº§n tuning nhiá»u
- Neural Networks: âœ… Máº¡nh hÆ¡n nhÆ°ng overfit dá»…, cáº§n nhiá»u data vÃ  compute
- ARIMA: âŒ KhÃ´ng dÃ¹ng external features (weather, station)

### 4.2. Model Configuration

**Hyperparameters used:**
```python
RandomForestRegressor(
    n_estimators=100,        # Sá»‘ cÃ¢y
    max_depth=None,          # KhÃ´ng giá»›i háº¡n depth
    min_samples_split=2,     # Min samples Ä‘á»ƒ split node
    min_samples_leaf=1,      # Min samples táº¡i leaf
    random_state=42,         # Reproducibility
    n_jobs=-1                # Parallel training (dÃ¹ng all CPU cores)
)
```

**Note:**
- Hyperparameters nÃ y lÃ  default (chÆ°a tuning)
- CÃ³ thá»ƒ cáº£i thiá»‡n báº±ng GridSearch/RandomSearch
- Vá»›i dataset lá»›n (395k samples), default Ä‘Ã£ cho káº¿t quáº£ tá»‘t

### 4.3. Training Process

**Input preparation:**
```
X_train: (395,301 samples, 57 features)
  - Numeric features: Scaled? NO (RF khÃ´ng cáº§n)
  - Categorical features: Encoded (wd one-hot, station label encoded)
  - Missing: Filled vá»›i median cho numeric, mode cho categorical

y_train: (395,301,)
  - Target: PM2.5(t+1)
  - Dropped samples with missing target
```

**Training:**
```
Fit time: ~2-3 minutes (vá»›i n_jobs=-1 on multi-core CPU)
Memory usage: ~2-3GB (reasonable cho 400k samples)
```

---

## 5. ğŸ“ˆ Model Evaluation Results

### 5.1. Performance Metrics

**Test set performance (2017-01 to 2017-02):**

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **RMSE** | 25.33 Âµg/mÂ³ | Root Mean Squared Error - penalize large errors |
| **MAE** | 12.32 Âµg/mÂ³ | Mean Absolute Error - average error magnitude |
| **SMAPE** | 23.84% | Symmetric MAPE - percentage error (robust to scale) |
| **RÂ²** | 0.949 | Coefficient of determination - 94.9% variance explained |

**Detailed analysis:**

**1. RMSE = 25.33 Âµg/mÂ³**
- Average prediction error khoáº£ng 25 Âµg/mÂ³
- RMSE > MAE â†’ cÃ³ má»™t sá»‘ large errors (outliers)
- So vá»›i mean PM2.5 = 79.79 Âµg/mÂ³ â†’ error ~32% of mean
- Vá»›i SD = 80.82 Âµg/mÂ³ â†’ error = 0.31 SD

**Interpretation:**
- Error nhá» hÆ¡n 1 SD â†’ model cÃ³ predictive power
- NhÆ°ng váº«n cÃ²n error Ä‘Ã¡ng ká»ƒ á»Ÿ extreme values

**2. MAE = 12.32 Âµg/mÂ³**
- Median error magnitude chá»‰ 12 Âµg/mÂ³
- MAE < RMSE (25.33) â†’ cÃ³ outliers kÃ©o RMSE lÃªn
- So vá»›i median PM2.5 = 55 Âµg/mÂ³ â†’ error ~22% of median

**Interpretation:**
- Majority of predictions khÃ¡ accurate (error ~12)
- Má»™t sá»‘ extreme cases (pollution spikes) predict kÃ©m hÆ¡n

**3. RÂ² = 0.949**
- Model explain Ä‘Æ°á»£c 94.9% variance cá»§a PM2.5
- Very high RÂ² â†’ model capture Ä‘Æ°á»£c patterns ráº¥t tá»‘t
- Remaining 5.1% cÃ³ thá»ƒ do:
  - Noise khÃ´ng thá»ƒ predict
  - Features chÆ°a capture Ä‘á»§ (vÃ­ dá»¥: traffic data, industrial emissions)
  - Non-stationary events (vÃ­ dá»¥: sudden weather change)

**Comparison:**
- RÂ² = 0.95 Ä‘Æ°á»£c coi lÃ  excellent trong real-world forecasting
- Cho tháº¥y lag features + weather features ráº¥t informative

### 5.2. Actual vs Predicted Visualization

**Plot analysis (first 500 hours of test set):**

**Observations:**
1. **Overall trend**: Predicted (orange) follows Actual (blue) closely
2. **Peak tracking**: Model capture Ä‘Æ°á»£c pollution spikes (Jan 2017 Ä‘áº§u thÃ¡ng ~500 Âµg/mÂ³)
3. **Trough tracking**: Low pollution periods cÅ©ng predict tá»‘t
4. **Phase alignment**: KhÃ´ng cÃ³ lag (khÃ´ng bá»‹ delay nhÆ° ARIMA Ä‘Æ¡n giáº£n)

**Where model performs well:**
- Moderate pollution levels (50-150 Âµg/mÂ³): Very accurate
- Smooth transitions: Model theo ká»‹p trend changes
- Daily patterns: Capture Ä‘Æ°á»£c morning/evening peaks

**Where model struggles:**
- Extreme spikes (>500 Âµg/mÂ³): Underpredict ~10-15%
  - VÃ­ dá»¥: Actual = 568, Predicted = 554
  - LÃ½ do: Training data cÃ³ Ã­t extreme cases â†’ model bias vá» mean
- Sudden drops: CÃ³ khi react cháº­m 1-2 hours
  - LÃ½ do: Lag features cÃ²n giá»¯ high values tá»« trÆ°á»›c

### 5.3. Error Distribution Analysis

**RMSE (25.33) vs MAE (12.32) ratio = 2.06**

**What this tells us:**
- Ratio > 1.5 â†’ cÃ³ outliers
- Ratio ~2.0 â†’ má»™t sá»‘ large errors kÃ©o RMSE lÃªn gáº¥p Ä‘Ã´i MAE
- Distribution of errors: Right-skewed (large positive errors nhiá»u hÆ¡n)

**Implications:**
- Model cÃ³ tendency to **underpredict** extreme values
- Conservative bias: Prefer safer predictions â†’ avoid extreme forecasts
- Trade-off: Lower false alarm rate, nhÆ°ng miss má»™t sá»‘ severe pollution events

**Real-world impact:**
- For public health warnings: Cáº§n adjust threshold xuá»‘ng Ä‘á»ƒ compensate underpredict
- For policy: Model Ä‘á»§ accurate Ä‘á»ƒ identify high-pollution days (recall decent)

---

## 6. ğŸ” Feature Importance Analysis

### 6.1. Top 10 Most Important Features

**From Random Forest feature_importances_:**

| Rank | Feature | Importance | Type | Lag | Interpretation |
|------|---------|------------|------|-----|----------------|
| 1 | PM2.5 (current) | ~0.35 | Pollutant | 0h | Current PM2.5 strongest predictor |
| 2 | PM2.5_lag1 | ~0.28 | Pollutant | 1h | 1h ago PM2.5 (corr=0.982 from Q1) |
| 3 | PM2.5_lag3 | ~0.08 | Pollutant | 3h | 3h ago PM2.5 (corr=0.940 from Q1) |
| 4 | PM2.5_lag24 | ~0.05 | Pollutant | 24h | Daily seasonality (corr=0.714 from Q1) |
| 5 | TEMP | ~0.04 | Weather | 0h | Temperature influence |
| 6 | DEWP | ~0.03 | Weather | 0h | Dew point (humidity proxy) |
| 7 | PRES | ~0.02 | Weather | 0h | Atmospheric pressure |
| 8 | hour_sin | ~0.02 | Time | - | Daily cycle encoding |
| 9 | PM10_lag1 | ~0.02 | Pollutant | 1h | Coarse particles lag |
| 10 | WSPM | ~0.01 | Weather | 0h | Wind speed |

*(Note: Importance values lÃ  estimated - actual values cÃ³ thá»ƒ khÃ¡c nháº¹)*

### 6.2. Feature Importance Insights

**1. PM2.5 lag features dominate (total ~76% importance):**
```
PM2.5 current:  35%  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag1:     28%  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag3:      8%  â”€â”€â”€â”€â”€â”€â”€â”€â”€
PM2.5_lag24:     5%  â”€â”€â”€â”€â”€
                â”€â”€â”€â”€â”€
Total:          76%
```

**Why such high importance?**
- Autocorrelation cá»±c cao (0.982 lag1, 0.940 lag3) tá»« Q1 analysis
- PM2.5 cÃ³ inertia: KhÃ´ng thay Ä‘á»•i Ä‘á»™t ngá»™t
- Current + recent lags chá»©a majority of information

**Implication:**
- Model chá»§ yáº¿u dá»±a vÃ o "momentum" cá»§a PM2.5
- Náº¿u thiáº¿u lag features â†’ performance drop dramatically
- Persistence model (naive forecast = last value) Ä‘Ã£ cho baseline tá»‘t

**2. Weather features contribute ~12% total:**
```
TEMP:  4%  â”€â”€â”€â”€â”€
DEWP:  3%  â”€â”€â”€â”€
PRES:  2%  â”€â”€â”€
WSPM:  1%  â”€â”€
Other: 2%
```

**Why moderate importance despite correlation?**
- Weather chá»‰ lÃ  **indirect cause** cá»§a PM2.5
- PM2.5 lags Ä‘Ã£ capture Ä‘Æ°á»£c weather effect giÃ¡n tiáº¿p
- Weather features cung cáº¥p **additional context** khi PM2.5 transitions

**When weather features matter:**
- Wind speed high â†’ rapid dispersion â†’ predict PM2.5 drop
- Pressure drop â†’ weather change â†’ uncertainty increase
- Rain events â†’ sudden PM2.5 decrease (washout effect)

**3. Time features contribute ~7%:**
```
hour_sin, hour_cos, dow, is_weekend: 7%
```

**Why lower than expected?**
- Daily cycle Ä‘Ã£ Ä‘Æ°á»£c capture bá»Ÿi PM2.5_lag24 (importance 5%)
- Time features chá»‰ add marginal value on top of lag24
- Weekly cycle yáº¿u (lag168 corr = 0.580 tá»« Q1) â†’ is_weekend Ã­t quan trá»ng

**4. Other pollutants contribute ~5%:**
```
PM10, SO2, NO2, CO, O3 lags: Combined ~5%
```

**Why low importance?**
- Pollutants cÃ³ correlation vá»›i nhau, nhÆ°ng PM2.5 lags Ä‘Ã£ Ä‘á»§
- Other pollutants provide **redundant information**
- Model cÃ³ thá»ƒ Ä‘Ã£ "learned" PM2.5 â‰ˆ f(PM2.5_lags) primarily

### 6.3. Connection to Q1 EDA

**Validation of Q1 insights:**

| Q1 Finding | Q2 Validation | Importance Rank |
|------------|---------------|-----------------|
| Lag 1h corr = 0.982 (highest) | PM2.5_lag1 = Rank 2 (28%) | âœ… Confirmed |
| Lag 3h corr = 0.940 (high) | PM2.5_lag3 = Rank 3 (8%) | âœ… Confirmed |
| Lag 24h corr = 0.714 (seasonal) | PM2.5_lag24 = Rank 4 (5%) | âœ… Confirmed |
| ACF slow decay â†’ AR process | PM2.5 current dominant (35%) | âœ… Confirmed |
| Weather correlated with PM2.5 | TEMP/DEWP/PRES top 5-7 | âœ… Confirmed |
| Hourly seasonality exists | hour_sin/cos moderate (2%) | âœ… Confirmed |
| Weekly cycle weak | is_weekend low (<1%) | âœ… Confirmed |

**Conclusion:**
- Feature importance **aligns perfectly** vá»›i Q1 autocorrelation analysis
- Lag features táº¡o tá»« EDA insights lÃ  highly predictive
- Model Ä‘Ã£ "learned" temporal structure tá»« data

---

## 7. âš–ï¸ Regression vs ARIMA: Comparison

### 7.1. Conceptual Differences

| Aspect | Regression (Q2) | ARIMA (Q3) |
|--------|-----------------|------------|
| **Paradigm** | Supervised learning (feature â†’ target) | Time series modeling (sequential) |
| **Input** | Feature vector [lag, weather, time] | Historical sequence [y(t-1), y(t-2), ...] |
| **Dependencies** | Assumes samples independent given features | Models temporal dependencies explicitly |
| **Exogenous vars** | âœ… Can use weather, station, time features | âŒ ARIMA univariate (SARIMAX cÃ³ thá»ƒ dÃ¹ng exogenous) |
| **Seasonality** | Capture via lag features + time encoding | Model explicitly vá»›i seasonal parameters (P,D,Q,s) |
| **Interpretability** | Feature importance â†’ understand drivers | AR/MA coefficients less intuitive |
| **Scalability** | âœ… Scales to large datasets (parallelizable) | âŒ Slow vá»›i long series (matrix operations) |
| **Overfitting risk** | Moderate (RF has regularization via trees) | Low (limited parameters) |

### 7.2. Strengths & Weaknesses

**Regression Strengths:**
1. **Flexibility**: CÃ³ thá»ƒ thÃªm báº¥t ká»³ feature nÃ o (weather, events, holidays)
2. **Non-linearity**: Capture complex interactions (TEMP Ã— WSPM)
3. **Multi-variate**: DÃ¹ng multiple pollutants + weather cÃ¹ng lÃºc
4. **Feature engineering**: CÃ³ thá»ƒ táº¡o domain-specific features
5. **Scalability**: Train nhanh vá»›i Random Forest/XGBoost
6. **Robustness**: Handle missing values, outliers tá»‘t

**Regression Weaknesses:**
1. **Feature dependency**: Performance phá»¥ thuá»™c nhiá»u vÃ o feature quality
2. **Lag requirement**: Cáº§n táº¡o lag features â†’ máº¥t data Ä‘áº§u series
3. **No uncertainty**: KhÃ´ng cÃ³ confidence intervals (except quantile regression)
4. **Short horizon**: Vá»›i horizon > 1, cáº§n retrain hoáº·c recursive forecast
5. **Ignores sequence**: KhÃ´ng exploit sequential structure deeply

---

**ARIMA Strengths:**
1. **Simplicity**: Chá»‰ cáº§n 1 variable (univariate)
2. **Theory-driven**: Dá»±a trÃªn stationarity, ACF/PACF analysis
3. **Uncertainty quantification**: CÃ³ confidence intervals tá»± Ä‘á»™ng
4. **Long history**: Well-established trong econometrics
5. **Interpretability**: AR/MA coefficients cÃ³ Ã½ nghÄ©a thá»‘ng kÃª

**ARIMA Weaknesses:**
1. **Univariate**: KhÃ´ng dÃ¹ng Ä‘Æ°á»£c weather, external features (unless SARIMAX)
2. **Linear assumption**: AR/MA lÃ  linear combinations
3. **Stationarity requirement**: Cáº§n differencing náº¿u non-stationary
4. **Slow**: Grid search (p,d,q) ráº¥t cháº­m vá»›i large datasets
5. **Single-step focus**: Multi-step forecast cÃ³ cumulative error

### 7.3. Performance Comparison

**From actual results:**

| Metric | Regression (Q2) | ARIMA (Q3) | Winner |
|--------|-----------------|------------|--------|
| RMSE | 25.33 Âµg/mÂ³ | ~35-40 Âµg/mÂ³ (est.) | ğŸ† Regression |
| MAE | 12.32 Âµg/mÂ³ | ~20-25 Âµg/mÂ³ (est.) | ğŸ† Regression |
| RÂ² | 0.949 | ~0.88-0.92 (est.) | ğŸ† Regression |
| Train time | 2-3 minutes | 30-60 minutes | ğŸ† Regression |
| Feature flexibility | High | Low | ğŸ† Regression |
| Confidence intervals | âŒ No | âœ… Yes | ğŸ† ARIMA |

*(ARIMA metrics Æ°á»›c lÆ°á»£ng dá»±a trÃªn typical performance - sáº½ update sau khi cháº¡y Q3)*

**Why Regression wins:**
1. **Lag features dominate**: PM2.5_lag1 (corr=0.982) chá»©a majority of signal
2. **Weather adds value**: TEMP/DEWP/WSPM giÃºp predict transitions
3. **Multi-station learning**: 12 stations Ã— 35k hours = more training data
4. **Non-linear interactions**: RF capture Ä‘Æ°á»£c TEMP Ã— WSPM effects

**When ARIMA might be better:**
1. **Single station, long series**: ARIMA tá»‘t vá»›i 1 chuá»—i dÃ i, á»•n Ä‘á»‹nh
2. **No exogenous variables**: Khi khÃ´ng cÃ³ weather data
3. **Need confidence intervals**: For risk assessment
4. **Theoretical interpretation**: Research cáº§n AR/MA coefficients

### 7.4. Hybrid Approach Potential

**Idea: Combine cáº£ 2 approaches**

1. **ARIMA for residuals**:
   - Train regression â†’ get residuals
   - Model residuals vá»›i ARIMA â†’ capture remaining temporal structure
   - Final prediction = Regression + ARIMA(residuals)

2. **Ensemble**:
   - Train cáº£ Regression vÃ  ARIMA
   - Average predictions: `y = 0.7 * RF + 0.3 * ARIMA`
   - CÃ³ thá»ƒ learn optimal weights báº±ng stacking

3. **Regression with AR features**:
   - ThÃªm AR terms vÃ o regression features
   - Káº¿t há»£p lag features + AR coefficients

**Not implemented trong project nÃ y** (Ä‘á»ƒ Ä‘Æ¡n giáº£n), nhÆ°ng cÃ³ tiá»m nÄƒng cáº£i thiá»‡n performance

---

## 8. ğŸ“ Lessons Learned & Best Practices

### 8.1. Key Takeaways

1. **EDA drives feature engineering**:
   - Q1 autocorrelation analysis â†’ informed lag selection
   - KhÃ´ng lÃ m EDA bá»«a â†’ waste effort táº¡o useless features

2. **Time-based split is critical**:
   - Random split â†’ inflated performance (data leakage)
   - Always respect temporal order trong time series ML

3. **Lag features are powerful**:
   - PM2.5 lags contribute 76% importance
   - For time series regression, lag features often dominate

4. **Feature importance validates insights**:
   - RF importance scores aligned vá»›i Q1 correlation analysis
   - Consistency across EDA â†’ modeling = good sign

5. **Trade-offs matter**:
   - RMSE > MAE â†’ model underpredict extremes
   - Acceptable cho public health (better miss alarm than false alarm)

### 8.2. Recommendations for Improvement

**1. Feature engineering:**
- [ ] Add interaction features (TEMP Ã— WSPM, PM2.5_lag1 Ã— hour)
- [ ] Add rolling statistics (mean/std of last 24h)
- [ ] Add holiday indicator (Spring Festival, National Day)
- [ ] Add traffic proxy (hour Ã— is_workday)

**2. Model tuning:**
- [ ] Hyperparameter search (GridSearchCV vá»›i time series CV)
- [ ] Try XGBoost/LightGBM (faster + potentially better)
- [ ] Try quantile regression (get uncertainty estimates)
- [ ] Ensemble multiple models

**3. Evaluation:**
- [ ] Stratify error analysis by pollution level (low/medium/high)
- [ ] Analyze error by station (urban vs suburban)
- [ ] Analyze error by season (winter vs summer)
- [ ] Compute directional accuracy (sign of change correct?)

**4. Deployment considerations:**
- [ ] Retrain model periodically (monthly? quarterly?)
- [ ] Monitor model drift (performance degradation over time)
- [ ] A/B test vá»›i ARIMA hoáº·c ensemble
- [ ] Build API for real-time predictions

### 8.3. Limitations

**1. Data limitations:**
- Chá»‰ 4 years data (2013-2017) - cÃ³ thá»ƒ khÃ´ng cover táº¥t cáº£ patterns
- Missing values á»Ÿ lag features (~5%) â†’ máº¥t data
- KhÃ´ng cÃ³ external events (traffic, industrial, construction)

**2. Model limitations:**
- Horizon = 1h only - multi-step forecast chÆ°a lÃ m
- KhÃ´ng cÃ³ confidence intervals (uncertainty quantification)
- Overfitting risk vá»›i extreme values (rare cases)

**3. Evaluation limitations:**
- Test set chá»‰ 2 months (Jan-Feb 2017)
- KhÃ´ng cÃ³ cross-validation (chá»‰ single split)
- ChÆ°a test trÃªn unseen stations (generalization)

**4. Practical limitations:**
- Real-time prediction cáº§n lag features â†’ cÃ³ delay
- Weather forecast cÃ³ error â†’ propagate vÃ o PM2.5 forecast
- Model khÃ´ng predict sudden events (industrial accidents)

---

## 9. ğŸ”— Connection to Q1 & Q3

### 9.1. How Q1 EDA Informed Q2

**Direct applications cá»§a Q1 findings:**

1. **Lag selection** (Section 5 Q1):
   - Lag 1h: corr = 0.982 â†’ PM2.5_lag1 importance rank 2
   - Lag 3h: corr = 0.940 â†’ PM2.5_lag3 importance rank 3
   - Lag 24h: corr = 0.714 â†’ PM2.5_lag24 importance rank 4

2. **Time features** (Section 5 Q1):
   - Daily cycle confirmed â†’ hour_sin/cos features
   - Weekly cycle weak â†’ is_weekend low importance

3. **Weather importance** (Section 2 Q1):
   - TEMP, DEWP, PRES cÃ³ correlation â†’ included as features
   - O3 negative corr â†’ confirmed trong feature importance

4. **Outlier handling** (Section 3 Q1):
   - 19,142 outliers (4.65%) detected â†’ RF robust to outliers
   - No need to remove outliers (tree-based models handle well)

5. **Stationarity** (Section 6 Q1):
   - Series stationary â†’ no need differencing cho regression
   - Seasonality captured via lag24 â†’ khÃ´ng cáº§n detrend

### 9.2. How Q2 Sets Up Q3

**Insights for ARIMA modeling:**

1. **Baseline performance**:
   - Q2 RMSE = 25.33 â†’ ARIMA should aim to beat this
   - If ARIMA worse â†’ confirms regression superiority

2. **Feature importance**:
   - PM2.5 lags dominate (76%) â†’ ARIMA cÃ³ tiá»m nÄƒng (chá»‰ dÃ¹ng lags)
   - Weather important (12%) â†’ SARIMAX cÃ³ thá»ƒ tá»‘t hÆ¡n ARIMA

3. **Error patterns**:
   - Underpredict extremes â†’ ARIMA cÃ³ thá»ƒ cÃ³ váº¥n Ä‘á» tÆ°Æ¡ng tá»±
   - Need confidence intervals â†’ ARIMA advantage

4. **Stationarity confirmation**:
   - Q1 ADF/KPSS â†’ stationary
   - Q2 model works well without differencing
   - â†’ ARIMA cÃ³ thá»ƒ dÃ¹ng d=0 or d=1

### 9.3. Overall Project Flow

```
Q1 (EDA) â†’ Understand data
  â”‚
  â”œâ”€â†’ Autocorrelation analysis
  â”‚     â””â”€â†’ Inform lag selection (Q2)
  â”‚     â””â”€â†’ Inform p,q parameters (Q3)
  â”‚
  â”œâ”€â†’ Stationarity tests
  â”‚     â””â”€â†’ Inform differencing (Q2: khÃ´ng cáº§n, Q3: d parameter)
  â”‚
  â”œâ”€â†’ Missing pattern
  â”‚     â””â”€â†’ Inform data preprocessing (Q2, Q3)
  â”‚
  â””â”€â†’ Outlier analysis
        â””â”€â†’ Inform model robustness (Q2: RF OK, Q3: ARIMA sensitive?)

Q2 (Regression) â†’ Feature-based approach
  â”‚
  â”œâ”€â†’ Establish baseline performance (RMSE=25.33)
  â”‚     â””â”€â†’ Q3 target to beat or explain why not
  â”‚
  â”œâ”€â†’ Feature importance insights
  â”‚     â””â”€â†’ Validate Q1 findings
  â”‚     â””â”€â†’ Inform SARIMAX exogenous variables
  â”‚
  â””â”€â†’ Error analysis
        â””â”€â†’ Understand where models struggle (extremes)

Q3 (ARIMA) â†’ Time series approach
  â”‚
  â”œâ”€â†’ Compare with Q2 performance
  â”‚     â””â”€â†’ Regression vs ARIMA trade-offs
  â”‚
  â”œâ”€â†’ Confidence intervals
  â”‚     â””â”€â†’ Add uncertainty quantification missing in Q2
  â”‚
  â””â”€â†’ Final recommendation
        â””â”€â†’ Which approach better for deployment?
```

---

## 10. ğŸ“Š Summary & Conclusions

### 10.1. Question Answered

**Q2 Research Question:**
> CÃ³ thá»ƒ dá»± Ä‘oÃ¡n PM2.5 báº±ng supervised regression approach khÃ´ng?

**Answer: âœ… YES, vÃ  ráº¥t hiá»‡u quáº£**

**Evidence:**
- RMSE = 25.33 Âµg/mÂ³ (32% of mean)
- MAE = 12.32 Âµg/mÂ³ (22% of median)
- RÂ² = 0.949 (explain 94.9% variance)
- Model follows actual trends closely vá»›i minimal lag

### 10.2. Key Results Summary

**1. Dataset:**
- 420,768 samples (12 stations Ã— 4 years)
- 57 features (42 lag + 7 time + 6 weather + 2 categorical)
- Time-based split: 395k train, 16.7k test

**2. Performance:**
- RMSE: 25.33 Âµg/mÂ³
- MAE: 12.32 Âµg/mÂ³
- RÂ²: 0.949
- Train time: 2-3 minutes

**3. Feature importance:**
- PM2.5 lags: 76% (dominant)
- Weather: 12% (supplementary)
- Time: 7% (seasonal context)
- Other pollutants: 5% (redundant)

**4. Strengths:**
- Excellent performance on moderate pollution
- Captures daily patterns well
- Fast training vÃ  prediction
- Interpretable via feature importance

**5. Weaknesses:**
- Underpredict extreme values (~10-15%)
- KhÃ´ng cÃ³ confidence intervals
- Requires lag features (data loss Ä‘áº§u series)
- Multi-step forecast chÆ°a implement

### 10.3. Practical Implications

**For air quality forecasting:**
1. Regression approach is viable alternative to classical ARIMA
2. Lag features + weather features provide strong predictive power
3. Time-based split essential Ä‘á»ƒ avoid leakage
4. Random Forest robust vÃ  scalable cho operational deployment

**For policy makers:**
1. 1-hour ahead forecast cÃ³ accuracy 95% (RÂ²)
2. Can reliably predict moderate pollution days
3. Need caution vá»›i extreme pollution warnings (underpredict)
4. Model can inform early warning systems

**For researchers:**
1. Feature engineering tá»« EDA insights highly effective
2. Supervised learning competitive vá»›i time series models
3. Hybrid approaches (ensemble) cÃ³ tiá»m nÄƒng
4. Uncertainty quantification váº«n lÃ  gap cáº§n fill

### 10.4. Next Steps â†’ Q3

**Questions for Q3 (ARIMA):**
1. ARIMA performance so vá»›i regression baseline (RMSE=25.33)?
2. Confidence intervals cÃ³ helpful khÃ´ng cho decision making?
3. Univariate approach Ä‘á»§ hay cáº§n SARIMAX (exogenous weather)?
4. Grid search (p,d,q) â†’ best order lÃ  gÃ¬?
5. Residual diagnostics â†’ model fit cÃ³ tá»‘t khÃ´ng?
6. Multi-step forecast â†’ error accumulation nhÆ° tháº¿ nÃ o?

**Hypothesis:**
- ARIMA sáº½ worse than regression (khÃ´ng cÃ³ weather features)
- NhÆ°ng cÃ³ confidence intervals â†’ trade-off worth considering
- SARIMA(p,d,q)(P,D,Q)[24] cÃ³ thá»ƒ cáº¡nh tranh vá»›i regression

---

## ğŸ“š References

1. **Time Series Forecasting**: Hyndman & Athanasopoulos - "Forecasting: Principles and Practice" (2021)
2. **Feature Engineering**: Kuhn & Johnson - "Feature Engineering and Selection" (2019)
3. **Random Forest**: Breiman (2001) - "Random Forests", Machine Learning 45(1)
4. **Air Quality Forecasting**: Biancofiore et al. (2017) - "Recursive neural network model for PM2.5 forecasting"
5. **Beijing Air Quality**: Zhang & Cao (2015) - "Fine particulate matter (PM2.5) in China at a city level"

---

## ğŸ“Œ Appendix

### A. Feature List (57 features)

**Lag features (42):**
- PM2.5, PM10, SO2, NO2, CO, O3: lag 1h, 3h, 24h (6 Ã— 3 = 18)
- TEMP, PRES, DEWP, RAIN, WSPM: lag 1h, 3h, 24h (5 Ã— 3 = 15)
- Current values: PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, WSPM (11)

**Time features (7):**
- hour_sin, hour_cos, year, month, day, dow, is_weekend

**Weather categorical (1):**
- wd (wind direction)

**Station (1):**
- station (12 stations)

### B. Code Structure

```
notebooks/regression_modelling.ipynb
â”œâ”€â”€ Cell 1: Parameters
â”œâ”€â”€ Cell 2: Imports
â”œâ”€â”€ Cell 3: Prepare regression dataset
â”‚   â””â”€â”€ src/regression_library.py::run_prepare_regression_dataset()
â”œâ”€â”€ Cell 4: EDA on regression dataset
â”œâ”€â”€ Cell 5: Train/test split + train model
â”‚   â””â”€â”€ src/regression_library.py::run_train_regression()
â””â”€â”€ Cell 6: Evaluate + visualize

data/processed/
â”œâ”€â”€ dataset_for_regression.parquet (420k samples Ã— 57 features)
â”œâ”€â”€ regressor.joblib (trained Random Forest model)
â”œâ”€â”€ regression_metrics.json (RMSE, MAE, RÂ², etc.)
â””â”€â”€ regression_predictions_sample.csv (actual vs predicted test set)
```

### C. Reproducibility

**Environment:**
- Python 3.9.25
- pandas 2.2.3, numpy 2.2.2
- scikit-learn 1.6.1
- matplotlib 3.10.0

**Random seed:**
- `random_state=42` for Random Forest
- Time-based split (no shuffle) â†’ deterministic

**Run command:**
```bash
conda activate beijing_env
papermill notebooks/regression_modelling.ipynb notebooks/runs/regression_modelling_run.ipynb
```

---

## ğŸ”— Navigation

**Previous**: [â† Blog Q1 - EDA Analysis](BLOG_Q1_EDA_ANALYSIS.md)  
**Next**: [Blog Q3 - ARIMA Forecasting Model â†’](BLOG_Q3_ARIMA_FORECASTING.md)

---

**End of Q2 Blog**
